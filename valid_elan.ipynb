{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "folder = 'IVAvalid_for_NCTU/day/c32b95371b6b-SOS20230226-143946-2887'\n",
    "img_json = '/TEST_images.json'\n",
    "with open(f'{folder}/TEST_images.json') as f:\n",
    "    img_paths = json.load(f)\n",
    "label_json = f'{folder}/TEST_objects.json'\n",
    "with open(label_json) as f:\n",
    "    label_dicts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models import vgg\n",
    "from torch_lib.Model_heading_bin import *\n",
    "from torch_lib.ClassAverages import *\n",
    "from torchvision import transforms\n",
    "import os, glob, cv2\n",
    "from library.ron_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_regressed_3d_bbox(img, cam_to_img, box_2d, dimensions, alpha, theta_ray, detectionid):\n",
    "\n",
    "    # the math! returns X, the corners used for constraint\n",
    "    location, X = calc_location(dimensions, cam_to_img, box_2d, alpha, theta_ray)\n",
    "\n",
    "    orient = alpha + theta_ray\n",
    "\n",
    "    #plot_2d_box(img, box_2d, detectionid)\n",
    "    plot_3d_box(img, cam_to_img, orient, dimensions, location) # 3d boxes\n",
    "\n",
    "    return location, orient\n",
    "\n",
    "weights_path = 'weights/DA_B4_N1_50.pkl'\n",
    "device = torch.device('cpu') # 選gpu的index\n",
    "checkpoint = torch.load(weights_path, map_location=device) #if training on 2 GPU, mapping on the same device\n",
    "normalize_type = checkpoint['normal']\n",
    "bin_num = checkpoint['bin']\n",
    "angle_per_class = 2*np.pi/float(bin_num)\n",
    "\n",
    "my_vgg = vgg.vgg19_bn(weights='DEFAULT').to(device)\n",
    "model = Model(features=my_vgg.features, bins=bin_num).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# for img processing\n",
    "if normalize_type == 0:\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "if normalize_type == 1:\n",
    "    normalize = transforms.Normalize(mean=[0.596, 0.612, 0.587], std=[0.256, 0.254, 0.257])\n",
    "process = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "\n",
    "ELAN_averages = ClassAverages(average_file='all_ELAN_class_averages.txt')\n",
    "cam_to_img = np.array([\n",
    "        [1.418667e+03, 0.000e+00, 6.4e+02, 0],\n",
    "        [0.000e+00, 1.418867e+03, 3.6e+02, 0],\n",
    "        [0.000e+00, 000e+00, 1.0e+00, 0] ])\n",
    "class_dict = {8:'truck', 9:'car', 10:'motor', 11:'bike'}\n",
    "\n",
    "# EVAL\n",
    "model.eval()\n",
    "for i in range(len(img_paths)):\n",
    "    path = os.path.join(folder, img_paths[i])\n",
    "    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    label_dict = label_dicts[i]\n",
    "    for j in range(len(label_dict['labels'])):\n",
    "        difficulty = label_dict['difficulties'][j] # 1 is hard to distinguish\n",
    "        if difficulty == 0:\n",
    "            color = (255, 0, 0)\n",
    "            line_width = 2\n",
    "        else:\n",
    "            color = (0, 0, 255)\n",
    "            line_width = 1\n",
    "        class_ = class_dict[label_dict['labels'][j]] # 5:行人,6:機車騎士,7:腳踏車騎士,8:大車,9:小車,10:機車,11:腳踏車\n",
    "        object_id = label_dict['id'][j] # for tracking\n",
    "        box_2d = label_dict['boxes'][j]\n",
    "        box_2d = [[box_2d[0], box_2d[1]], [box_2d[2],box_2d[3]]]\n",
    "        left_top = box_2d[0]\n",
    "        right_btm = box_2d[1]\n",
    "        #cv2.rectangle(img, left_top, right_btm, color, line_width)\n",
    "        crop = img[left_top[1]:right_btm[1]+1, left_top[0]:right_btm[0]+1] \n",
    "        crop = cv2.resize(src = crop, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        crop = process(crop) # expand to 224x224\n",
    "        crop = torch.stack([crop]).to(device)\n",
    "        \n",
    "        [RESIDUALs, BIN_CONFs, delta_DIMs] = model(crop)\n",
    "        bin_argmax = torch.max(BIN_CONFs, dim=1)[1]\n",
    "        orient_residual = RESIDUALs[torch.arange(len(RESIDUALs)), bin_argmax] \n",
    "        Alphas = angle_per_class*bin_argmax + orient_residual #mapping bin_class and residual to get alpha\n",
    "        alpha_Elan = float(Alphas[0].data)\n",
    "        alpha_Elan = angle_correction(alpha_Elan)\n",
    "        dim_Elan = delta_DIMs.cpu().data.numpy()[0, :]\n",
    "        dim_Elan += ELAN_averages.get_item(class_)\n",
    "        \n",
    "        theta_ray = calc_theta_ray(img.shape[1], box_2d, cam_to_img)\n",
    "        loc, ry = plot_regressed_3d_bbox(img, cam_to_img, box_2d, dim_Elan, alpha_Elan, theta_ray, j)\n",
    "        \n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(len(renew_labels)):\n",
    "    img = cv2.imread(images[i])\n",
    "    lines = [x.strip() for x in open(renew_labels[i]).readlines()]\n",
    "    label_ELAN = ''\n",
    "    for idx, line in enumerate(lines):\n",
    "        #print(line)\n",
    "        elements = line.split()\n",
    "        for j in range(1, len(elements)):\n",
    "            elements[j] = float(elements[j])\n",
    "\n",
    "        class_ = elements[0]\n",
    "        truncate = elements[1]\n",
    "        occluded = elements[2]\n",
    "\n",
    "        top_left = (int(round(elements[4])), int(round(elements[5])))\n",
    "        btm_right = (int(round(elements[6])), int(round(elements[7])))\n",
    "        box_2d = (top_left, btm_right)\n",
    "        dim_gt = [elements[8], elements[9], elements[10]] # height, width, length\n",
    "        crop = img[top_left[1]:btm_right[1]+1, top_left[0]:btm_right[0]+1] \n",
    "        crop = cv2.resize(src = crop, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        crop = process(crop) # expand to 224x224\n",
    "        #2dbox\n",
    "        crop = torch.stack([crop]).to(device)\n",
    "        #Location = [elements[11], elements[12], elements[13]]\n",
    "\n",
    "        alpha_gt = elements[3]\n",
    "        ry_gt = elements[14]\n",
    "        theta_ray = ry_gt - alpha_gt\n",
    "\n",
    "        [RESIDUALs, BIN_CONFs, delta_DIMs] = model(crop)\n",
    "        bin_argmax = torch.max(BIN_CONFs, dim=1)[1]\n",
    "        orient_residual = RESIDUALs[torch.arange(len(RESIDUALs)), bin_argmax] \n",
    "        Alphas = angle_per_class*bin_argmax + orient_residual #mapping bin_class and residual to get alpha\n",
    "        alpha_Elan = float(Alphas[0].data)\n",
    "        alpha_Elan = angle_correction(alpha_Elan)\n",
    "        dim_Elan = delta_DIMs.cpu().data.numpy()[0, :]\n",
    "        dim_Elan += ELAN_averages.get_item(class_)\n",
    "\n",
    "        loc, ry = plot_regressed_3d_bbox(img, cam_to_img, box_2d, dim_Elan, alpha_Elan, theta_ray, idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
