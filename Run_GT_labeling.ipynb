{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95c2f07",
   "metadata": {},
   "source": [
    "# FASTER Get Pred Labels (40 min->10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cd47f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg\n",
    "from torch_lib.Model import *\n",
    "from torch_lib.ClassAverages import *\n",
    "from torchvision import transforms\n",
    "import os, glob, cv2\n",
    "from library.ron_utils import *\n",
    "\n",
    "def get_calibration_cam_to_image(cab_f):\n",
    "    for line in open(cab_f):\n",
    "        if 'P2:' in line:\n",
    "            cam_to_img = line.strip().split(' ')\n",
    "            cam_to_img = np.asarray([float(number) for number in cam_to_img[1:]])\n",
    "            cam_to_img = np.reshape(cam_to_img, (3, 4))\n",
    "            return cam_to_img\n",
    "\n",
    "def Run_GT_pred_labels(weights_path, pred_label_root, cuda=0):\n",
    "    device = torch.device(f'cuda:{cuda}')\n",
    "    os.makedirs(pred_label_root, exist_ok=True)\n",
    "    my_vgg = vgg.vgg19_bn(pretrained=True)\n",
    "    model = Model(features=my_vgg.features, bins=2).to(device)\n",
    "    \n",
    "    #因為train的時候在不同GPU上，eval要map到同個GPU https://www.jianshu.com/p/ec91b3b59f66\n",
    "    checkpoint = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    # for img processing\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    process = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "    # Kitti image_2 dir / label_2 dir\n",
    "    img_root = \"./Kitti/training/image_2\"\n",
    "    label_root = \"./Kitti/training/label_2\"\n",
    "    calib_root = \"./Kitti/training/calib\"\n",
    "    extra_label_root = \"./Kitti/training/extra_label\"\n",
    "    ImageSets_root = './Kitti/ImageSets'\n",
    "    split = 'trainval'\n",
    "\n",
    "    images = glob.glob(os.path.join(img_root, '*.png'), recursive=True)\n",
    "    labels = glob.glob(os.path.join(label_root, '*.txt'), recursive=True)\n",
    "    calibs = glob.glob(os.path.join(calib_root, '*.txt'), recursive=True)\n",
    "    extra = glob.glob(os.path.join(extra_label_root, '*.txt'), recursive=True)\n",
    "\n",
    "    split_dir = os.path.join(ImageSets_root, split + '.txt')\n",
    "    ids = [int(x.strip()) for x in open(split_dir).readlines()]\n",
    "\n",
    "    # dim averages\n",
    "    averages_all = ClassAverages()\n",
    "    start = time.time()\n",
    "    for i in ids:\n",
    "        img = cv2.imread(images[i])\n",
    "        img_W = img.shape[1]\n",
    "        cam_to_img = get_calibration_cam_to_image(calibs[i])\n",
    "\n",
    "        CLASSes = list()\n",
    "        TRUNCATEDs = list()\n",
    "        OCCLUDEDs = list()\n",
    "        BOX2Ds = list()\n",
    "        CROPs_tensor = list()\n",
    "        Alphas = list()\n",
    "        THETAs = list()\n",
    "        depth_GT = list()\n",
    "        extra_labels = get_extra_labels(extra[i])\n",
    "\n",
    "        with open(labels[i]) as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            for idx, line in enumerate(lines):\n",
    "                elements = line[:-1].split()\n",
    "                if elements[0] == 'DontCare':\n",
    "                    continue\n",
    "                for j in range(1, len(elements)):\n",
    "                    elements[j] = float(elements[j])\n",
    "\n",
    "                CLASSes.append(elements[0])\n",
    "                TRUNCATEDs.append(elements[1])\n",
    "                OCCLUDEDs.append(elements[2])\n",
    "                top_left = (int(round(elements[4])), int(round(elements[5])))\n",
    "                btm_right = (int(round(elements[6])), int(round(elements[7])))\n",
    "                box = [top_left, btm_right]\n",
    "                BOX2Ds.append(box)\n",
    "                #cv2 is(H,W,3)\n",
    "                crop = img[top_left[1]:btm_right[1]+1, top_left[0]:btm_right[0]+1] \n",
    "                crop = cv2.resize(src = crop, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "                crop = process(crop) # expand to 224x224\n",
    "\n",
    "                # Use 'calc_theta_ray(img_width, box, proj_matrix)', if cam_to_img changes to proj_matrix:\"camera_cal/calib_cam_to_cam.txt\"\n",
    "                theta_ray = extra_labels[idx]['Theta_ray']\n",
    "                THETAs.append(theta_ray)\n",
    "                depth_label = elements[13]\n",
    "                depth_GT.append(depth_label)\n",
    "                #4dim\n",
    "                if is_cond:\n",
    "                    cond = torch.tensor(theta_ray).expand(1, crop.shape[1], crop.shape[2])\n",
    "                    img_cond = torch.concat((crop, cond), dim=0) # 3+1, 224, 224\n",
    "                    CROPs_tensor.append(img_cond)\n",
    "                else:\n",
    "                    CROPs_tensor.append(crop)\n",
    "\n",
    "            # put together as a batch\n",
    "            input_ = torch.stack(CROPs_tensor).to(device)\n",
    "            # model regress part\n",
    "            [RESIDUALs, BIN_CONFs, delta_DIMs, depth_BIAs] = model(input_)\n",
    "\n",
    "            bin_argmax = torch.max(BIN_CONFs, dim=1)[1]\n",
    "            orient_residual = RESIDUALs[torch.arange(len(RESIDUALs)), bin_argmax] \n",
    "            Alphas = angle_per_class*bin_argmax + orient_residual #mapping bin_class and residual to get alpha\n",
    "\n",
    "        #write pred_label.txt \n",
    "        with open(labels[i].replace(label_root, pred_label_root),'w') as new_f:\n",
    "            pred_labels = ''\n",
    "            for class_, truncated, occluded, delta, alpha, theta, box_2d, bias, gt in zip(CLASSes, TRUNCATEDs, OCCLUDEDs, delta_DIMs, Alphas, THETAs, BOX2Ds, depth_BIAs, depth_GT):\n",
    "                delta = delta.cpu().data #torch->numpy\n",
    "                alpha = alpha.cpu().data #torch->numpy\n",
    "                bias = bias.cpu().data\n",
    "                if alpha > np.pi:\n",
    "                    alpha -= (2*np.pi) #for fitting val-range\n",
    "                dim = delta + averages_all.get_item(class_)\n",
    "                rotation_y = alpha + theta\n",
    "                loc, _ = calc_location(dim, cam_to_img, box_2d, alpha, theta)\n",
    "\n",
    "                calc_depth = loc[2]\n",
    "                depth_width = calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, dim[1], dim[2], alpha, truncated)\n",
    "                regress_depth = depth_width + bias\n",
    "                print(f'Calc:', calc_depth, 'GT', gt)\n",
    "                print(f'Width:{depth_width:.2f}, bias:', bias)\n",
    "\n",
    "                \n",
    "                pred_labels += '{CLASS} {T:.1f} {O} {A:.2f} {left} {top} {right} {btm} {H:.2f} {W:.2f} {L:.2f} {X:.2f} {Y:.2f} {Z:.2f} {Ry:.2f}\\n'.format(\n",
    "                    CLASS=class_, T=truncated, O=occluded, A=alpha, left=box_2d[0][0], top=box_2d[0][1], right=box_2d[1][0], btm=box_2d[1][1],\n",
    "                    H=dim[0], W=dim[1], L=dim[2], X=loc[0], Y=loc[1], Z=loc[2], Ry=rotation_y)\n",
    "            #print(pred_labels)\n",
    "            new_f.writelines(pred_labels)\n",
    "            \n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "    print('Done, take {} min {} sec'.format((time.time()-start)//60, (time.time()-start)%60))# around 10min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf97df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "Done, take 8.0 min 36.863417863845825 sec\n"
     ]
    }
   ],
   "source": [
    "weights_path = 'weights_group/cuda0_w03_up10_100.pkl'\n",
    "pred_label_root = 'cuda0_G_100'\n",
    "Run_GT_pred_labels(weights_path, pred_label_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb893e",
   "metadata": {},
   "source": [
    "## 0606 Doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2fc1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg\n",
    "from torch_lib.Model_heading_bin import *\n",
    "from torch_lib.ClassAverages import *\n",
    "from torchvision import transforms\n",
    "import os, glob, cv2\n",
    "from library.ron_utils import *\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cuda = 0\n",
    "pred_label_root = '0606_depth_bias'\n",
    "bin_num=32\n",
    "angle_per_class = 2*np.pi/float(bin_num)\n",
    "\n",
    "is_cond = False\n",
    "#weights_path = 'weights/depth_bin32_group_cond_epoch_100.pkl'\n",
    "#weights_path = 'weights/depth_bin32_group_cond_no_warm_epoch_100.pkl'\n",
    "weights_path = 'weights/depth_bin32_group_epoch_100.pkl'\n",
    "\n",
    "device = torch.device(f'cuda:{cuda}')\n",
    "os.makedirs(pred_label_root, exist_ok=True)\n",
    "my_vgg = vgg.vgg19_bn(pretrained=True)\n",
    "if is_cond:\n",
    "    print(\"< add Condition (4-dim) as input >\")\n",
    "    my_vgg.features[0] = nn.Conv2d(4, 64, (3,3), (1,1), (1,1))\n",
    "\n",
    "model = Model(features=my_vgg.features, bins=bin_num).to(device)\n",
    "\n",
    "#因為train的時候在不同GPU上，eval要map到同個GPU https://www.jianshu.com/p/ec91b3b59f66\n",
    "checkpoint = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "# for img processing\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "process = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "img_root = \"./Kitti/training/image_2\"\n",
    "label_root = \"./Kitti/training/label_2\"\n",
    "calib_root = \"./Kitti/training/calib\"\n",
    "extra_label_root = \"./Kitti/training/extra_label\"\n",
    "ImageSets_root = './Kitti/ImageSets'\n",
    "split = 'trainval'\n",
    "\n",
    "images = glob.glob(os.path.join(img_root, '*.png'), recursive=True)\n",
    "labels = glob.glob(os.path.join(label_root, '*.txt'), recursive=True)\n",
    "calibs = glob.glob(os.path.join(calib_root, '*.txt'), recursive=True)\n",
    "extra = glob.glob(os.path.join(extra_label_root, '*.txt'), recursive=True)\n",
    "\n",
    "split_dir = os.path.join(ImageSets_root, split + '.txt')\n",
    "ids = [int(x.strip()) for x in open(split_dir).readlines()]\n",
    "\n",
    "# dim averages\n",
    "averages_all = ClassAverages()\n",
    "start = time.time()\n",
    "\n",
    "DELTA_ORG = list()\n",
    "DELTA_NEW = list()\n",
    "DELTA_NEW_GT_Alpha = list()\n",
    "DELTA_NEW_GT_W = list()\n",
    "DELTA_NEW_GT_L = list()\n",
    "DELTA_NEW_GT_Dim = list()\n",
    "DELTA_NEW_GT_All = list()\n",
    "delta_ALL = list()\n",
    "for i in ids:\n",
    "    img = cv2.imread(images[i])\n",
    "    img_W = img.shape[1] # H,W,3\n",
    "    cam_to_img = get_calibration_cam_to_image(calibs[i])\n",
    "\n",
    "    CLASSes = list()\n",
    "    TRUNCATEDs = list()\n",
    "    OCCLUDEDs = list()\n",
    "    BOX2Ds = list()\n",
    "    CROPs_tensor = list()\n",
    "    Alphas = list()\n",
    "    THETAs = list()\n",
    "    Depth_GT = list()\n",
    "    Dim_GT = list()\n",
    "    Alpha_GT = list()\n",
    "    extra_labels = get_extra_labels(extra[i])\n",
    "\n",
    "    with open(labels[i]) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        elements = line[:-1].split()\n",
    "        if elements[0] != 'Car':\n",
    "            continue\n",
    "        for j in range(1, len(elements)):\n",
    "            elements[j] = float(elements[j])\n",
    "        \n",
    "        truncate = elements[1] # 0~1\n",
    "        occluded = int(elements[2]) # 0:visible, 1:partly, 2:largely, 3:unknown\n",
    "\n",
    "        if truncate<=0.3 and occluded<=1:\n",
    "\n",
    "            CLASSes.append(elements[0])\n",
    "            TRUNCATEDs.append(elements[1])\n",
    "            OCCLUDEDs.append(elements[2])\n",
    "            top_left = (int(round(elements[4])), int(round(elements[5])))\n",
    "            btm_right = (int(round(elements[6])), int(round(elements[7])))\n",
    "            box_2d = [top_left, btm_right]\n",
    "            BOX2Ds.append(box_2d)\n",
    "            \n",
    "            dim = np.array([elements[8], elements[9], elements[10]], dtype=np.double) # Height, Width, Length\n",
    "            Dim_GT.append(dim)\n",
    "            alpha = elements[3]\n",
    "            Alpha_GT.append(alpha)\n",
    "            #cv2 is(H,W,3)\n",
    "            crop = img[top_left[1]:btm_right[1]+1, top_left[0]:btm_right[0]+1] \n",
    "            crop = cv2.resize(src = crop, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "            crop = process(crop) # expand to 224x224\n",
    "\n",
    "            # Use 'calc_theta_ray(img_width, box, proj_matrix)', if cam_to_img changes to proj_matrix:\"camera_cal/calib_cam_to_cam.txt\"\n",
    "            theta_ray = extra_labels[idx]['Theta_ray']\n",
    "            THETAs.append(theta_ray)\n",
    "            depth_label = elements[13]\n",
    "            Depth_GT.append(depth_label)\n",
    "            #4dim\n",
    "            if is_cond:\n",
    "                #cond = torch.tensor(theta_ray).expand(1, crop.shape[1], crop.shape[2]) theta as cond\n",
    "                boxW_ratio = extra_labels[idx]['Box_W']/224.\n",
    "                cond = torch.tensor(boxW_ratio).expand(1, crop.shape[1], crop.shape[2]) #boxH ratio as cond\n",
    "                img_cond = torch.concat((crop, cond), dim=0) # 3+1, 224, 224\n",
    "                CROPs_tensor.append(img_cond)\n",
    "            else:\n",
    "                CROPs_tensor.append(crop)\n",
    "\n",
    "            # plot box\n",
    "            '''\n",
    "            d2_center = get_box_center(box_2d)\n",
    "            cv2.rectangle(img, box_2d[0], box_2d[1], color=(255,0,0), thickness=2)\n",
    "            cv2.putText(img, f'{idx}', (d2_center[0], d2_center[1]+10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    '''\n",
    "    if len(CROPs_tensor) == 0:\n",
    "        continue\n",
    "    # put together as a batch\n",
    "    input_ = torch.stack(CROPs_tensor).to(device)\n",
    "    # model regress part\n",
    "    [RESIDUALs, BIN_CONFs, delta_DIMs, depth_BIAS] = model(input_)\n",
    "\n",
    "    bin_argmax = torch.max(BIN_CONFs, dim=1)[1]\n",
    "    orient_residual = RESIDUALs[torch.arange(len(RESIDUALs)), bin_argmax] \n",
    "    Alphas = angle_per_class*bin_argmax + orient_residual #mapping bin_class and residual to get alpha\n",
    "\n",
    "    #write pred_label.txt \n",
    "    #with open(labels[i].replace(label_root, pred_label_root),'w') as new_f:\n",
    "    pred_labels = ''\n",
    "    for class_, truncated, occluded, delta, alpha, theta, box_2d, bias, depth_gt, alpha_gt, dim_gt in zip(CLASSes, TRUNCATEDs, OCCLUDEDs, delta_DIMs, Alphas, THETAs, BOX2Ds, depth_BIAS, Depth_GT, Alpha_GT, Dim_GT):\n",
    "        delta = delta.cpu().data #torch->numpy\n",
    "        alpha = alpha.cpu().data #torch->numpy\n",
    "        bias = bias.cpu().data\n",
    "        if alpha > np.pi:\n",
    "            alpha -= (2*np.pi) #for fitting val-range\n",
    "        dim = delta + averages_all.get_item(class_)\n",
    "        rotation_y = alpha + theta\n",
    "        loc, _ = calc_location(dim, cam_to_img, box_2d, alpha, theta)\n",
    "        \n",
    "        # COMPARE \n",
    "        org_calc_depth = loc[2]\n",
    "        depth_width = calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, dim[1], dim[2], alpha, truncated)\n",
    "        '''\n",
    "        depth_width_GT_alpha = calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, dim[1], dim[2], alpha_gt, truncated)\n",
    "        depth_width_GT_w = calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, dim_gt[1], dim[2], alpha, truncated)\n",
    "        depth_width_GT_l = calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, dim[1], dim_gt[2], alpha, truncated)\n",
    "        depth_width_GT_dim = calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, dim_gt[1], dim_gt[2], alpha, truncated)\n",
    "        '''\n",
    "        depth_width_GT_all = calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, dim_gt[1], dim_gt[2], alpha_gt, truncated)\n",
    "        \n",
    "        delta_ALL.append(depth_width-depth_width_GT_all)\n",
    "        '''\n",
    "        regress_depth = depth_width + float(bias)\n",
    "        regress_depth_GT_alpha = depth_width_GT_alpha + float(bias)\n",
    "        regress_depth_GT_w = depth_width_GT_w + float(bias)\n",
    "        regress_depth_GT_l = depth_width_GT_l + float(bias)\n",
    "        regress_depth_GT_dim = depth_width_GT_dim + float(bias)\n",
    "        regress_depth_GT_all = depth_width_GT_all + float(bias)\n",
    "        #print(f'GT:{gt:.3f} Calc: {org_calc_depth:.3f}, NEW:{regress_depth:.3f}, W:{depth_width:.3f}, bias:{float(bias):.3f}')\n",
    "        DELTA_ORG.append(depth_gt-org_calc_depth)\n",
    "        DELTA_NEW.append(depth_gt-regress_depth)\n",
    "        DELTA_NEW_GT_Alpha.append(depth_gt-regress_depth_GT_alpha)\n",
    "        DELTA_NEW_GT_W.append(depth_gt-regress_depth_GT_w)\n",
    "        DELTA_NEW_GT_L.append(depth_gt-regress_depth_GT_l)\n",
    "        DELTA_NEW_GT_Dim.append(depth_gt-regress_depth_GT_dim)\n",
    "        DELTA_NEW_GT_All.append(depth_gt-regress_depth_GT_all)\n",
    "        pred_labels += '{CLASS} {T:.1f} {O} {A:.2f} {left} {top} {right} {btm} {H:.2f} {W:.2f} {L:.2f} {X:.2f} {Y:.2f} {Z:.2f} {Ry:.2f}\\n'.format(\n",
    "            CLASS=class_, T=truncated, O=occluded, A=alpha, left=box_2d[0][0], top=box_2d[0][1], right=box_2d[1][0], btm=box_2d[1][1],\n",
    "            H=dim[0], W=dim[1], L=dim[2], X=loc[0], Y=loc[1], Z=loc[2], Ry=rotation_y)\n",
    "        '''\n",
    "    #print(pred_labels)\n",
    "        #new_f.writelines(pred_labels)\n",
    "    if i%500==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "73cac966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg\n",
    "from torch_lib.Model_heading_bin import *\n",
    "from torch_lib.ClassAverages import *\n",
    "from torchvision import transforms\n",
    "import os, glob, cv2\n",
    "from library.ron_utils import *\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cuda = 0\n",
    "pred_label_root = '0606_depth_bias'\n",
    "bin_num=32\n",
    "angle_per_class = 2*np.pi/float(bin_num)\n",
    "\n",
    "is_cond = False\n",
    "#weights_path = 'weights/depth_bin32_group_cond_epoch_100.pkl'\n",
    "#weights_path = 'weights/depth_bin32_group_cond_no_warm_epoch_100.pkl'\n",
    "weights_path = 'weights/depth_bin32_group_epoch_100.pkl'\n",
    "\n",
    "device = torch.device(f'cuda:{cuda}')\n",
    "os.makedirs(pred_label_root, exist_ok=True)\n",
    "my_vgg = vgg.vgg19_bn(pretrained=True)\n",
    "if is_cond:\n",
    "    print(\"< add Condition (4-dim) as input >\")\n",
    "    my_vgg.features[0] = nn.Conv2d(4, 64, (3,3), (1,1), (1,1))\n",
    "\n",
    "model = Model(features=my_vgg.features, bins=bin_num).to(device)\n",
    "\n",
    "#因為train的時候在不同GPU上，eval要map到同個GPU https://www.jianshu.com/p/ec91b3b59f66\n",
    "checkpoint = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "# for img processing\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "process = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "img_root = \"./Kitti/training/image_2\"\n",
    "label_root = \"./Kitti/training/label_2\"\n",
    "calib_root = \"./Kitti/training/calib\"\n",
    "extra_label_root = \"./Kitti/training/extra_label\"\n",
    "ImageSets_root = './Kitti/ImageSets'\n",
    "split = 'trainval'\n",
    "\n",
    "images = glob.glob(os.path.join(img_root, '*.png'), recursive=True)\n",
    "labels = glob.glob(os.path.join(label_root, '*.txt'), recursive=True)\n",
    "calibs = glob.glob(os.path.join(calib_root, '*.txt'), recursive=True)\n",
    "extra = glob.glob(os.path.join(extra_label_root, '*.txt'), recursive=True)\n",
    "\n",
    "split_dir = os.path.join(ImageSets_root, split + '.txt')\n",
    "ids = [int(x.strip()) for x in open(split_dir).readlines()]\n",
    "\n",
    "# dim averages\n",
    "averages_all = ClassAverages()\n",
    "start = time.time()\n",
    "\n",
    "depth_ORG = list()\n",
    "depth_bias = list()\n",
    "# regressed\n",
    "DIM_REG = list()\n",
    "Alpha_REG = list()\n",
    "\n",
    "# GT labels\n",
    "IMG_W = list()\n",
    "TRUNCATEDs = list()\n",
    "BOX2Ds = list()\n",
    "depth_GT = list()\n",
    "DIM_GT = list()\n",
    "Alpha_GT = list()\n",
    "\n",
    "for i in ids:\n",
    "    img = cv2.imread(images[i])\n",
    "    img_W = img.shape[1] # H,W,3\n",
    "    cam_to_img = get_calibration_cam_to_image(calibs[i])\n",
    "\n",
    "    CROPs_tensor = list()\n",
    "    THETAs = list()\n",
    "    extra_labels = get_extra_labels(extra[i])\n",
    "\n",
    "    with open(labels[i]) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        elements = line[:-1].split()\n",
    "        if elements[0] != 'Car':\n",
    "            continue\n",
    "        for j in range(1, len(elements)):\n",
    "            elements[j] = float(elements[j])\n",
    "        \n",
    "        truncate = elements[1] # 0~1\n",
    "        occluded = int(elements[2]) # 0:visible, 1:partly, 2:largely, 3:unknown\n",
    "\n",
    "        if truncate<=0.3 and occluded<=1:\n",
    "            IMG_W.append(img_W)\n",
    "            TRUNCATEDs.append(elements[1])\n",
    "            top_left = (int(round(elements[4])), int(round(elements[5])))\n",
    "            btm_right = (int(round(elements[6])), int(round(elements[7])))\n",
    "            box_2d = [top_left, btm_right]\n",
    "            BOX2Ds.append(box_2d)\n",
    "            \n",
    "            dim = np.array([elements[8], elements[9], elements[10]], dtype=np.double) # Height, Width, Length\n",
    "            DIM_GT.append(dim.tolist())\n",
    "            alpha = elements[3]\n",
    "            Alpha_GT.append(alpha)\n",
    "            #cv2 is(H,W,3)\n",
    "            crop = img[top_left[1]:btm_right[1]+1, top_left[0]:btm_right[0]+1] \n",
    "            crop = cv2.resize(src = crop, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "            crop = process(crop) # expand to 224x224\n",
    "\n",
    "            # Use 'calc_theta_ray(img_width, box, proj_matrix)', if cam_to_img changes to proj_matrix:\"camera_cal/calib_cam_to_cam.txt\"\n",
    "            theta_ray = extra_labels[idx]['Theta_ray']\n",
    "            THETAs.append(theta_ray)\n",
    "            depth_label = elements[13]\n",
    "            depth_GT.append(depth_label)\n",
    "            #4dim\n",
    "            if is_cond:\n",
    "                #cond = torch.tensor(theta_ray).expand(1, crop.shape[1], crop.shape[2]) theta as cond\n",
    "                boxW_ratio = extra_labels[idx]['Box_W'] / 224.\n",
    "                cond = torch.tensor(boxW_ratio).expand(1, crop.shape[1], crop.shape[2]) #boxH ratio as cond\n",
    "                img_cond = torch.concat((crop, cond), dim=0) # 3+1, 224, 224\n",
    "                CROPs_tensor.append(img_cond)\n",
    "            else:\n",
    "                CROPs_tensor.append(crop)\n",
    "\n",
    "            # plot box\n",
    "            '''\n",
    "            d2_center = get_box_center(box_2d)\n",
    "            cv2.rectangle(img, box_2d[0], box_2d[1], color=(255,0,0), thickness=2)\n",
    "            cv2.putText(img, f'{idx}', (d2_center[0], d2_center[1]+10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    '''\n",
    "    if len(CROPs_tensor) == 0:\n",
    "        continue\n",
    "    # put together as a batch\n",
    "    input_ = torch.stack(CROPs_tensor).to(device)\n",
    "    # model regress part\n",
    "    [RESIDUALs, BIN_CONFs, delta_DIMs, depth_BIAS] = model(input_)\n",
    "\n",
    "    bin_argmax = torch.max(BIN_CONFs, dim=1)[1]\n",
    "    orient_residual = RESIDUALs[torch.arange(len(RESIDUALs)), bin_argmax] \n",
    "    Alphas = angle_per_class*bin_argmax + orient_residual #mapping bin_class and residual to get alpha\n",
    "\n",
    "    for delta, alpha, theta, bias in zip (delta_DIMs, Alphas, THETAs, depth_BIAS):\n",
    "        delta = delta.cpu().data.numpy() #torch->numpy\n",
    "        alpha = alpha.cpu().data.numpy() #torch->numpy\n",
    "        bias = bias.cpu().item()\n",
    "        if alpha > np.pi:\n",
    "            alpha -= (2*np.pi) #for fitting val-range\n",
    "        dim = delta + averages_all.get_item('Car')\n",
    "        loc, _ = calc_location(dim, cam_to_img, box_2d, alpha, theta)\n",
    "        #\n",
    "        DIM_REG.append(dim.tolist())\n",
    "        Alpha_REG.append(alpha)\n",
    "        \n",
    "        # COMPARE \n",
    "        depth_ORG.append(loc[2])\n",
    "        #depth_calc_REG.append(calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, dim[1], dim[2], alpha, trun))\n",
    "        #depth_calc_GT.append(calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, dim_gt[1], dim_gt[2], alpha_gt, trun))\n",
    "        depth_bias.append(bias)\n",
    "        \n",
    "    if i%500==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, obj_W, obj_L, alpha, trun=0.0):\n",
    "    fovx = 2 * np.arctan(img_W / (2 * cam_to_img[0][0]))\n",
    "    box_W = get_box_size(box_2d)[0] / (1-trun+0.01) #assume truncate related to W only\n",
    "    visual_W = abs(obj_L*np.cos(alpha)) + abs(obj_W*np.sin(alpha))\n",
    "    theta_ray = calc_theta_ray(img_W, box_2d, cam_to_img)\n",
    "    visual_W /= abs(np.cos(theta_ray)) #new added !\n",
    "    Wview = (visual_W)*(img_W/box_W)\n",
    "    depth = Wview/2 / np.tan(fovx/2)\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "45f42ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_GT = np.array(DIM_GT)\n",
    "DIM_REG = np.array(DIM_REG)\n",
    "Alpha_REG = np.array(Alpha_REG)\n",
    "depth_bias = np.array(depth_bias)\n",
    "depth_GT = np.array(depth_GT)\n",
    "\n",
    "\n",
    "depth_bias_REG = list()\n",
    "depth_bias_GT = list()\n",
    "depth_bias_GT_dim = list()\n",
    "depth_bias_GT_alpha = list()\n",
    "\n",
    "for bias, cam_to_img, imgW, trun, box_2d, a_reg, dim_reg, a_gt, dim_gt in zip(depth_bias, CAM, IMG_W, TRUNCATEDs, BOX2Ds, Alpha_REG, DIM_REG, Alpha_GT, DIM_GT):\n",
    "    depth_bias_REG.append(bias + calc_depth_with_alpha_theta(imgW, box_2d, cam_to_img, dim_reg[1], dim_reg[2], a_reg, trun))\n",
    "    depth_bias_GT_dim.append(bias + calc_depth_with_alpha_theta(imgW, box_2d, cam_to_img, dim_gt[1], dim_gt[2], a_reg, trun))\n",
    "    depth_bias_GT_alpha.append(bias + calc_depth_with_alpha_theta(imgW, box_2d, cam_to_img, dim_reg[1], dim_reg[2], a_gt, trun))\n",
    "    depth_bias_GT.append(bias + calc_depth_with_alpha_theta(imgW, box_2d, cam_to_img, dim_gt[1], dim_gt[2], a_gt, trun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "7637fd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34057819675166795\n",
      "1.930732374583325\n",
      "1.3629656899195657\n",
      "1.4009110227787773\n"
     ]
    }
   ],
   "source": [
    "print(abs(depth_bias_GT - depth_GT).mean())\n",
    "print(abs(depth_bias_REG - depth_GT).mean())\n",
    "print(abs(depth_bias_GT_dim - depth_GT).mean())\n",
    "print(abs(depth_bias_GT_alpha - depth_GT).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "74c5fabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07031175746413798\n",
      "1.9999999997868825\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "radian_delta = (1-np.cos(Alpha_GT - Alpha_REG))\n",
    "plt.hist(radian_delta)\n",
    "plt.plot()\n",
    "print(radian_delta.mean())\n",
    "print(radian_delta.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "56203eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.51\n",
      "1.631572\n"
     ]
    }
   ],
   "source": [
    "max_idx = np.where(radian_delta == radian_delta.max())[0][0]\n",
    "print(Alpha_GT[max_idx])\n",
    "print(Alpha_REG[max_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "ba965daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.51\n",
      "40.99\n",
      "GT alpha\n",
      "40.73253649549734\n",
      "REG alpha\n",
      "46.941765105712726\n"
     ]
    }
   ],
   "source": [
    "print(Alpha_GT[max_idx])\n",
    "print(depth_GT[max_idx])\n",
    "print('GT alpha')\n",
    "print(calc_depth_with_alpha_theta(IMG_W[max_idx], BOX2Ds[max_idx], CAM[max_idx], DIM_GT[max_idx][1], DIM_GT[max_idx][2], Alpha_GT[max_idx], TRUNCATEDs[max_idx]))\n",
    "print('REG alpha')\n",
    "print(calc_depth_with_alpha_theta(IMG_W[max_idx], BOX2Ds[max_idx], CAM[max_idx], DIM_GT[max_idx][1], DIM_GT[max_idx][2], Alpha_GT[max_idx]+0.07, TRUNCATEDs[max_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "e68ef658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.56 4.08\n"
     ]
    }
   ],
   "source": [
    "print(dim_gt[1], dim_gt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a642e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59.45014993 34.07181707 11.73868168 38.40881382 51.46045647 31.90104884\n",
      " 20.11304859 39.18915677 24.23409767 47.90959162 61.61231115  7.71389245\n",
      " 14.39797917 33.4362789  19.78650277 24.06628866 69.6762475  11.6145346\n",
      " 16.1488987  23.95815037]\n",
      "[57.07036805 27.91338194 11.90904911 37.95768479 57.64307585 23.89386906\n",
      " 22.30641019 35.55363494 23.83529919 51.76439445 61.52170301  7.61619586\n",
      " 12.86343196 27.66182235 19.13104261 21.29364123 74.26851543 11.17236168\n",
      " 16.18445746 23.09508821]\n"
     ]
    }
   ],
   "source": [
    "print(depth_W_GT[:20])\n",
    "print(depth_W[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8d2fc5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_idx = np.array([1,4,5,7,9,12,13,15,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "8779ca63",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [402], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mAlpha_GT\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlarge_idx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(Alpha_REG[large_idx])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m((Alpha_GT[large_idx] \u001b[38;5;241m-\u001b[39m Alpha_REG[large_idx])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m180\u001b[39m\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi)\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "print(Alpha_GT[large_idx])\n",
    "print(Alpha_REG[large_idx])\n",
    "print((Alpha_GT[large_idx] - Alpha_REG[large_idx])*180/np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "be213bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11266037 -0.04507118  0.09808223]\n",
      " [-0.10013493  0.13550494 -0.99848109]\n",
      " [ 0.14108546  0.02386729  0.12686823]\n",
      " [ 0.14878839  0.04871505  0.28087215]\n",
      " [-0.09541136 -0.08635528 -0.044276  ]\n",
      " [-0.04600539  0.04078437  0.21999673]\n",
      " [ 0.10170231  0.01504478 -0.26026581]\n",
      " [ 0.1378584   0.07999014  0.12673943]\n",
      " [-0.0713496  -0.10199367 -0.57571523]]\n"
     ]
    }
   ],
   "source": [
    "print(Dim_GT[large_idx] - Dim_REG[large_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3e36bfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.41 1.58 4.36]\n",
      " [1.38 1.8  3.41]\n",
      " [1.67 1.64 4.32]\n",
      " [1.68 1.67 4.29]\n",
      " [1.4  1.51 3.7 ]\n",
      " [1.47 1.6  3.66]\n",
      " [1.7  1.63 4.08]\n",
      " [1.61 1.66 3.2 ]\n",
      " [1.41 1.53 3.37]]\n",
      "[[1.52266037 1.62507118 4.26191777]\n",
      " [1.48013493 1.66449506 4.40848109]\n",
      " [1.52891454 1.61613271 4.19313177]\n",
      " [1.53121161 1.62128495 4.00912785]\n",
      " [1.49541136 1.59635528 3.744276  ]\n",
      " [1.51600539 1.55921563 3.44000327]\n",
      " [1.59829769 1.61495522 4.34026581]\n",
      " [1.4721416  1.58000986 3.07326057]\n",
      " [1.4813496  1.63199367 3.94571523]]\n"
     ]
    }
   ],
   "source": [
    "print(Dim_GT[large_idx])\n",
    "print(Dim_REG[large_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "23b2641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008114653639495373\n",
      "0.3417843282222748\n",
      "0.012190095148980618\n",
      "0.03446679934859276\n",
      "0.006173649337142706\n",
      "0.017392834648489952\n",
      "0.026102671399712563\n",
      "0.013822086155414581\n",
      "0.11564719676971436\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "for idx in large_idx:\n",
    "    print(F.mse_loss(torch.Tensor(Dim_GT[idx]), torch.Tensor(Dim_REG[idx])).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3832f432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08527135848999023\n",
      "0.4113736152648926\n",
      "0.09727362543344498\n",
      "0.15945856273174286\n",
      "0.07534758001565933\n",
      "0.1022622212767601\n",
      "0.12567098438739777\n",
      "0.11486268043518066\n",
      "0.24968619644641876\n"
     ]
    }
   ],
   "source": [
    "for idx in large_idx:\n",
    "    print(F.l1_loss(torch.Tensor(Dim_GT[idx]), torch.Tensor(Dim_REG[idx])).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "787ac756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1859)\n",
      "tensor(0.1579)\n"
     ]
    }
   ],
   "source": [
    "def L1_loss_alpha(input, target, alpha, device='cuda:0'):\n",
    "    weights = [torch.ones(input.shape[0]).to(device), 1+torch.sin(alpha)**2, 1+torch.cos(alpha)**2]\n",
    "    weights = torch.stack(weights, dim=1).to(device)\n",
    "    loss = abs(input-target)\n",
    "    loss *= weights\n",
    "    return torch.mean(loss)\n",
    "\n",
    "print(L1_loss_alpha(torch.Tensor(Dim_GT[large_idx]), torch.Tensor(Dim_REG[large_idx]), torch.Tensor(Alpha_GT[large_idx])))\n",
    "print(F.l1_loss(torch.Tensor(Dim_GT[large_idx]), torch.Tensor(Dim_REG[large_idx]), reduction='mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3c4b125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1127, 0.0451, 0.0981],\n",
      "        [0.1001, 0.1355, 0.9985],\n",
      "        [0.1411, 0.0239, 0.1269],\n",
      "        [0.1488, 0.0487, 0.2809],\n",
      "        [0.0954, 0.0864, 0.0443],\n",
      "        [0.0460, 0.0408, 0.2200],\n",
      "        [0.1017, 0.0150, 0.2603],\n",
      "        [0.1379, 0.0800, 0.1267],\n",
      "        [0.0713, 0.1020, 0.5757]])\n",
      "W\n",
      "tensor([[1.0000, 1.9902, 1.0098],\n",
      "        [1.0000, 1.9074, 1.0926],\n",
      "        [1.0000, 1.8754, 1.1246],\n",
      "        [1.0000, 1.7874, 1.2126],\n",
      "        [1.0000, 1.9807, 1.0193],\n",
      "        [1.0000, 1.9431, 1.0569],\n",
      "        [1.0000, 1.9716, 1.0284],\n",
      "        [1.0000, 1.9950, 1.0050],\n",
      "        [1.0000, 1.9569, 1.0431]])\n",
      "CALC\n",
      "tensor([[0.1127, 0.0897, 0.0990],\n",
      "        [0.1001, 0.2585, 1.0909],\n",
      "        [0.1411, 0.0448, 0.1427],\n",
      "        [0.1488, 0.0871, 0.3406],\n",
      "        [0.0954, 0.1710, 0.0451],\n",
      "        [0.0460, 0.0792, 0.2325],\n",
      "        [0.1017, 0.0297, 0.2676],\n",
      "        [0.1379, 0.1596, 0.1274],\n",
      "        [0.0713, 0.1996, 0.6005]])\n",
      "tensor(0.1859)\n"
     ]
    }
   ],
   "source": [
    "input_ = torch.Tensor(Dim_GT[large_idx])\n",
    "target = torch.Tensor(Dim_REG[large_idx])\n",
    "alphas_ = torch.Tensor(Alpha_GT[large_idx])\n",
    "weights = [torch.ones(input_.shape[0]), 1+torch.sin(alphas_)**2, 1+torch.cos(alphas_)**2]\n",
    "weights = torch.stack(weights, dim=1)\n",
    "delta = abs(input_-target)\n",
    "print(delta)\n",
    "print('W')\n",
    "print(weights)\n",
    "print('CALC')\n",
    "print(delta*weights)\n",
    "print(torch.mean(delta*weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "900a5cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.2888)\n",
      "tensor(38.6523)\n",
      "tensor(51.9718)\n"
     ]
    }
   ],
   "source": [
    "print(calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, Dim_GT[4][1], Dim_GT[4][2], alpha, 0))\n",
    "print(calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, Dim_GT[4][1], Dim_GT[4][2]+1, alpha, 0))\n",
    "print(calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, Dim_GT[4][1]+1, Dim_GT[4][2], alpha, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b1c9c",
   "metadata": {},
   "source": [
    "### Cond pkl (depth_bin32_group_cond_epoch_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5a542d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9433497171688714\n",
      "3.0952088386106587\n",
      "40570\n"
     ]
    }
   ],
   "source": [
    "# all objects\n",
    "DELTA_ORG = np.array(DELTA_ORG)\n",
    "DELTA_NEW = np.array(DELTA_NEW)\n",
    "\n",
    "print(abs(DELTA_ORG).mean())\n",
    "print(abs(DELTA_NEW).mean())\n",
    "print(len(DELTA_ORG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d038e14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.428124641751901\n",
      "3.143497436865388\n",
      "27237\n"
     ]
    }
   ],
   "source": [
    "# all objects truncate<=0.3, occuluded<=1\n",
    "DELTA_ORG = np.array(DELTA_ORG)\n",
    "DELTA_NEW = np.array(DELTA_NEW)\n",
    "\n",
    "print(abs(DELTA_ORG).mean())\n",
    "print(abs(DELTA_NEW).mean())\n",
    "print(len(DELTA_ORG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62f50a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1461617629105847\n",
      "2.2121288069672596\n",
      "19250\n"
     ]
    }
   ],
   "source": [
    "# Cars truncate<=0.3, occuluded<=1\n",
    "DELTA_ORG = np.array(DELTA_ORG)\n",
    "DELTA_NEW = np.array(DELTA_NEW)\n",
    "\n",
    "print(abs(DELTA_ORG).mean())\n",
    "print(abs(DELTA_NEW).mean())\n",
    "print(len(DELTA_ORG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d106164",
   "metadata": {},
   "source": [
    "### No cond pkl (depth_bin32_group_epoch_100) 沒有cond的效果目前比較好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f88fab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9208071008763112\n",
      "1.9307324017285972\n",
      "19250\n"
     ]
    }
   ],
   "source": [
    "# Cars truncate<=0.3, occuluded<=1\n",
    "DELTA_ORG = np.array(DELTA_ORG)\n",
    "DELTA_NEW = np.array(DELTA_NEW)\n",
    "\n",
    "print(abs(DELTA_ORG).mean())\n",
    "print(abs(DELTA_NEW).mean())\n",
    "print(len(DELTA_ORG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d74448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org calc: 1.921\n",
      "regress: 1.931\n",
      "GT alpha: 1.401\n",
      "GT W: 1.575\n",
      "GT L: 1.692\n",
      "GT dim: 1.363\n",
      "GT all: 0.341\n",
      "19250\n"
     ]
    }
   ],
   "source": [
    "# Cars truncate<=0.3, occuluded<=1\n",
    "DELTA_ORG = np.array(DELTA_ORG)\n",
    "DELTA_NEW = np.array(DELTA_NEW)\n",
    "DELTA_NEW_GT_Alpha = np.array(DELTA_NEW_GT_Alpha)\n",
    "DELTA_NEW_GT_W = np.array(DELTA_NEW_GT_W)\n",
    "DELTA_NEW_GT_L = np.array(DELTA_NEW_GT_L)\n",
    "DELTA_NEW_GT_Dim = np.array(DELTA_NEW_GT_Dim)\n",
    "DELTA_NEW_GT_All = np.array(DELTA_NEW_GT_All)\n",
    "\n",
    "print(f'org calc: {abs(DELTA_ORG).mean():.3f}')\n",
    "print(f'regress: {abs(DELTA_NEW).mean():.3f}') # 全部都用regressed value\n",
    "print(f'GT alpha: {abs(DELTA_NEW_GT_Alpha).mean():.3f}') # GT alpha\n",
    "print(f'GT W: {abs(DELTA_NEW_GT_W).mean():.3f}') # GT dim W\n",
    "print(f'GT L: {abs(DELTA_NEW_GT_L).mean():.3f}') # GT dim L\n",
    "print(f'GT dim: {abs(DELTA_NEW_GT_Dim).mean():.3f}') # GT W, L\n",
    "print(f'GT all: {abs(DELTA_NEW_GT_All).mean():.3f}') #如果是用計算的dim, alpha 會GT會精準很多 (受到很多其他影響)\n",
    "print(len(DELTA_ORG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59f718",
   "metadata": {},
   "source": [
    "# 0613 Dimension loss comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "4dbb8ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg\n",
    "from torch_lib.Model_heading_bin_no_bias import *\n",
    "from torch_lib.ClassAverages import *\n",
    "from torchvision import transforms\n",
    "import os, glob, cv2\n",
    "from library.ron_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cuda = 0\n",
    "#pred_label_root = '0606_depth_bias'\n",
    "bin_num=32\n",
    "angle_per_class = 2*np.pi/float(bin_num)\n",
    "\n",
    "W_MSE = 'weights/mse_loss_epoch_20.pkl'\n",
    "W_L1 = 'weights/L1_loss_epoch_20.pkl'\n",
    "W_AL1 = 'weights/Alpha_L1_loss_epoch_20.pkl'\n",
    "device = 'cuda:0'\n",
    "#os.makedirs(pred_label_root, exist_ok=True)\n",
    "my_vgg = vgg.vgg19_bn(pretrained=True)\n",
    "model_MSE = Model(features=my_vgg.features, bins=bin_num).to(device)\n",
    "#因為train的時候在不同GPU上，eval要map到同個GPU https://www.jianshu.com/p/ec91b3b59f66\n",
    "model_MSE.load_state_dict(torch.load(W_MSE, map_location=device)['model_state_dict'])\n",
    "model_MSE.eval()\n",
    "\n",
    "model_L1 = Model(features=my_vgg.features, bins=bin_num).to(device)\n",
    "model_L1.load_state_dict(torch.load(W_L1, map_location=device)['model_state_dict'])\n",
    "model_L1.eval()\n",
    "\n",
    "model_AL1 = Model(features=my_vgg.features, bins=bin_num).to(device)\n",
    "model_AL1.load_state_dict(torch.load(W_AL1, map_location=device)['model_state_dict'])\n",
    "model_AL1.eval()\n",
    "# for img processing\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "process = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "img_root = \"./Kitti/training/image_2\"\n",
    "label_root = \"./Kitti/training/label_2\"\n",
    "calib_root = \"./Kitti/training/calib\"\n",
    "extra_label_root = \"./Kitti/training/extra_label\"\n",
    "images = glob.glob(os.path.join(img_root, '*.png'), recursive=True)\n",
    "labels = glob.glob(os.path.join(label_root, '*.txt'), recursive=True)\n",
    "calibs = glob.glob(os.path.join(calib_root, '*.txt'), recursive=True)\n",
    "extra = glob.glob(os.path.join(extra_label_root, '*.txt'), recursive=True)\n",
    "\n",
    "ImageSets_root = './Kitti/ImageSets'\n",
    "split = 'trainval'\n",
    "split_dir = os.path.join(ImageSets_root, split + '.txt')\n",
    "ids = [int(x.strip()) for x in open(split_dir).readlines()]\n",
    "\n",
    "# dim averages\n",
    "averages_all = ClassAverages()\n",
    "\n",
    "depth_GT = list()\n",
    "imgW_GT = list()\n",
    "Trun_GT = list()\n",
    "BOX2D_GT = list()\n",
    "Alpha_GT = list()\n",
    "DIM_GT = list()  \n",
    "DIM_REG_MSE = list()\n",
    "DIM_REG_L1 = list()\n",
    "DIM_REG_AL1 = list()\n",
    "CAM = list()\n",
    "for i in ids:\n",
    "    img = cv2.imread(images[i])\n",
    "    imgW = img.shape[1] # H,W,3\n",
    "    cam_to_img = get_calibration_cam_to_image(calibs[i])\n",
    "    extra_labels = get_extra_labels(extra[i])\n",
    "    CROPs_tensor = list()\n",
    "    with open(labels[i]) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        elements = line[:-1].split()\n",
    "        \n",
    "        for j in range(1, len(elements)):\n",
    "            elements[j] = float(elements[j])\n",
    "        \n",
    "        truncate = elements[1] # 0~1\n",
    "        occluded = int(elements[2]) # 0:visible, 1:partly, 2:largely, 3:unknown\n",
    "        \n",
    "        if elements[0] != 'Car' or truncate>0.3 or occluded>1:\n",
    "            continue\n",
    "        CAM.append(cam_to_img)\n",
    "        imgW_GT.append(imgW)\n",
    "        Trun_GT.append(truncate)\n",
    "        top_left = (int(round(elements[4])), int(round(elements[5])))\n",
    "        btm_right = (int(round(elements[6])), int(round(elements[7])))\n",
    "        box_2d = [top_left, btm_right]\n",
    "        BOX2D_GT.append(box_2d)\n",
    "\n",
    "        dim = np.array([elements[8], elements[9], elements[10]], dtype=np.double) # Height, Width, Length\n",
    "        DIM_GT.append(dim)\n",
    "        alpha = elements[3]\n",
    "        Alpha_GT.append(alpha)\n",
    "        #cv2 is(H,W,3)\n",
    "        crop = img[top_left[1]:btm_right[1]+1, top_left[0]:btm_right[0]+1] \n",
    "        crop = cv2.resize(src = crop, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        crop = process(crop) # expand to 224x224\n",
    "        theta_ray = extra_labels[idx]['Theta_ray']\n",
    "        THETAs.append(theta_ray)\n",
    "        depth_label = elements[13]\n",
    "        depth_GT.append(depth_label)\n",
    "        CROPs_tensor.append(crop)\n",
    "        '''\n",
    "        d2_center = get_box_center(box_2d)\n",
    "        cv2.rectangle(img, box_2d[0], box_2d[1], color=(255,0,0), thickness=2)\n",
    "        cv2.putText(img, f'{idx}', (d2_center[0], d2_center[1]+10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    '''\n",
    "    if len(CROPs_tensor) == 0:\n",
    "        continue\n",
    "    # put together as a batch\n",
    "    input_ = torch.stack(CROPs_tensor).to(device)\n",
    "    \n",
    "    # MSE model regress\n",
    "    [RESIDUALs, BIN_CONFs, delta_MSE] = model_MSE(input_)\n",
    "    [RESIDUALs, BIN_CONFs, delta_L1] = model_L1(input_)\n",
    "    [RESIDUALs, BIN_CONFs, delta_AL1] = model_AL1(input_)\n",
    "\n",
    "    for mse, l1, al1 in zip(delta_MSE, delta_L1, delta_AL1):\n",
    "        mse = mse.cpu().data.numpy() #torch->numpy\n",
    "        l1 = l1.cpu().data.numpy() #torch->numpy\n",
    "        al1 = al1.cpu().data.numpy() #torch->numpy\n",
    "        DIM_REG_MSE.append(mse + averages_all.get_item('Car'))\n",
    "        DIM_REG_L1.append(l1 + averages_all.get_item('Car'))\n",
    "        DIM_REG_AL1.append(al1 + averages_all.get_item('Car'))\n",
    "        #calc_depth_with_alpha_theta(img_W, box_2d, cam_to_img, dim[1], dim[2], alpha, trun)\n",
    "    \n",
    "    if i%500==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ae3078e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_GT = np.array(DIM_GT)\n",
    "DIM_REG_MSE = np.array(DIM_REG_MSE)\n",
    "DIM_REG_L1 = np.array(DIM_REG_L1)\n",
    "DIM_REG_AL1 = np.array(DIM_REG_AL1)\n",
    "depth_GT = np.array(depth_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d7f7f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_depth_GT = list()\n",
    "calc_depth_MSE = list()\n",
    "calc_depth_L1 = list()\n",
    "calc_depth_AL1 = list()\n",
    "for cam_to_img, imgW, trun, box_2d, alpha, dim_gt, dim_mse, dim_l1, dim_al1 in zip(CAM, imgW_GT, Trun_GT, BOX2D_GT, Alpha_GT, DIM_GT, DIM_REG_MSE, DIM_REG_L1, DIM_REG_AL1):\n",
    "    calc_depth_GT.append(calc_depth_with_alpha_theta(imgW, box_2d, cam_to_img, dim_gt[1], dim_gt[2], alpha, trun))\n",
    "    calc_depth_MSE.append(calc_depth_with_alpha_theta(imgW, box_2d, cam_to_img, dim_mse[1], dim_mse[2], alpha, trun))\n",
    "    calc_depth_L1.append(calc_depth_with_alpha_theta(imgW, box_2d, cam_to_img, dim_l1[1], dim_l1[2], alpha, trun))\n",
    "    calc_depth_AL1.append(calc_depth_with_alpha_theta(imgW, box_2d, cam_to_img, dim_al1[1], dim_al1[2], alpha, trun))\n",
    "calc_depth_GT = np.array(calc_depth_GT)\n",
    "calc_depth_MSE = np.array(calc_depth_MSE)\n",
    "calc_depth_L1 = np.array(calc_depth_L1)\n",
    "calc_depth_AL1 = np.array(calc_depth_AL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "65867644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_depth_error_calculation(depth_labels, depth_Calcs, out_range=10):\n",
    "    class_GT = np.copy(depth_labels) #28742 car\n",
    "    print(f'num of Car:', class_GT.shape[0])\n",
    "    class_cal = np.copy(depth_Calcs)\n",
    "    for depth in [0, 10, 20, 30, 40, 50]:\n",
    "        class_GT_depth = class_GT[np.logical_and(class_GT >= depth, class_GT < depth+10.)]\n",
    "        print(f'\\tnum of depth {depth}-{depth+10}:', class_GT_depth.shape[0], end=' ')\n",
    "        class_cal_depth = class_cal[np.logical_and(class_GT >= depth, class_GT < depth+10.)]\n",
    "        cal_delta = abs(class_GT_depth - class_cal_depth)\n",
    "        #cal_delta, _, out_indexes = filter_out_of_range(cal_delta, out_range) # remove prediction out of 10\n",
    "        print(f'\\tabs_delta mean:{cal_delta.mean():.3f}m, Out of {out_range}m: {cal_delta[cal_delta>=out_range].shape[0]}')\n",
    "\n",
    "    # after 60 m\n",
    "    class_GT_depth = class_GT[class_GT >= 60.]\n",
    "    print(f'\\tnum of depth {depth+10}+:', class_GT_depth.shape[0], end='   ')\n",
    "    class_cal_depth = class_cal[class_GT >= 60.]\n",
    "    cal_delta = abs(class_GT_depth - class_cal_depth)\n",
    "    #cal_delta, _, out_indexes = filter_out_of_range(cal_delta, out_range) # remove prediction out of 10\n",
    "    print(f'\\tabs_delta mean:{cal_delta.mean():.3f}m, Out of {out_range}m: {cal_delta[cal_delta>=out_range].shape[0]}')\n",
    "    \n",
    "    total = abs(class_GT-class_cal)\n",
    "    print(f'[Total] mean:{total.mean():.3f}, std:{total.std():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "e94369be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of Car: 19250\n",
      "\tnum of depth 0-10: 1321 \tabs_delta mean:0.664m, Out of 5m: 0\n",
      "\tnum of depth 10-20: 4479 \tabs_delta mean:0.254m, Out of 5m: 0\n",
      "\tnum of depth 20-30: 4357 \tabs_delta mean:0.375m, Out of 5m: 0\n",
      "\tnum of depth 30-40: 3600 \tabs_delta mean:0.442m, Out of 5m: 0\n",
      "\tnum of depth 40-50: 2702 \tabs_delta mean:0.575m, Out of 5m: 0\n",
      "\tnum of depth 50-60: 1619 \tabs_delta mean:0.737m, Out of 5m: 0\n",
      "\tnum of depth 60+: 1172   \tabs_delta mean:1.067m, Out of 5m: 0\n",
      "[Total] mean:0.480, std:0.522\n"
     ]
    }
   ],
   "source": [
    "box_depth_error_calculation(depth_GT, calc_depth_GT, out_range=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "ff728d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of Car: 19250\n",
      "\tnum of depth 0-10: 1321 \tabs_delta mean:0.793m, Out of 5m: 0\n",
      "\tnum of depth 10-20: 4479 \tabs_delta mean:0.733m, Out of 5m: 3\n",
      "\tnum of depth 20-30: 4357 \tabs_delta mean:1.233m, Out of 5m: 23\n",
      "\tnum of depth 30-40: 3600 \tabs_delta mean:1.624m, Out of 5m: 100\n",
      "\tnum of depth 40-50: 2702 \tabs_delta mean:2.130m, Out of 5m: 171\n",
      "\tnum of depth 50-60: 1619 \tabs_delta mean:2.880m, Out of 5m: 264\n",
      "\tnum of depth 60+: 1172   \tabs_delta mean:3.700m, Out of 5m: 321\n",
      "[Total] mean:1.574, std:1.728\n"
     ]
    }
   ],
   "source": [
    "box_depth_error_calculation(depth_GT, calc_depth_MSE, out_range=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "9bdcebc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of Car: 19250\n",
      "\tnum of depth 0-10: 1321 \tabs_delta mean:0.777m, Out of 5m: 0\n",
      "\tnum of depth 10-20: 4479 \tabs_delta mean:0.713m, Out of 5m: 3\n",
      "\tnum of depth 20-30: 4357 \tabs_delta mean:1.170m, Out of 5m: 15\n",
      "\tnum of depth 30-40: 3600 \tabs_delta mean:1.588m, Out of 5m: 95\n",
      "\tnum of depth 40-50: 2702 \tabs_delta mean:2.034m, Out of 5m: 153\n",
      "\tnum of depth 50-60: 1619 \tabs_delta mean:2.789m, Out of 5m: 242\n",
      "\tnum of depth 60+: 1172   \tabs_delta mean:3.631m, Out of 5m: 303\n",
      "[Total] mean:1.522, std:1.708\n"
     ]
    }
   ],
   "source": [
    "box_depth_error_calculation(depth_GT, calc_depth_L1, out_range=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "c8b1b8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of Car: 19250\n",
      "\tnum of depth 0-10: 1321 \tabs_delta mean:0.778m, Out of 5m: 0\n",
      "\tnum of depth 10-20: 4479 \tabs_delta mean:0.681m, Out of 5m: 3\n",
      "\tnum of depth 20-30: 4357 \tabs_delta mean:1.125m, Out of 5m: 18\n",
      "\tnum of depth 30-40: 3600 \tabs_delta mean:1.509m, Out of 5m: 80\n",
      "\tnum of depth 40-50: 2702 \tabs_delta mean:1.944m, Out of 5m: 142\n",
      "\tnum of depth 50-60: 1619 \tabs_delta mean:2.691m, Out of 5m: 219\n",
      "\tnum of depth 60+: 1172   \tabs_delta mean:3.498m, Out of 5m: 278\n",
      "[Total] mean:1.461, std:1.659\n"
     ]
    }
   ],
   "source": [
    "box_depth_error_calculation(depth_GT, calc_depth_AL1, out_range=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "731282e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL\n",
      "0.14393166116461553\n",
      "0.13814717896766388\n",
      "0.134547621557227\n",
      "Val\n",
      "0.14448681662514817\n",
      "0.13857501836045888\n",
      "0.13504707924301218\n",
      "Train\n",
      "0.14563839353193608\n",
      "0.1388124312541294\n",
      "0.1352747878371857\n"
     ]
    }
   ],
   "source": [
    "print('ALL')\n",
    "print(abs(DIM_GT[:,:]-DIM_REG_MSE[:,:]).mean())\n",
    "print(abs(DIM_GT[:,:]-DIM_REG_L1[:,:]).mean())\n",
    "print(abs(DIM_GT[:,:]-DIM_REG_AL1[:,:]).mean())\n",
    "print('Val')\n",
    "print(abs(DIM_GT[val_ids,:]-DIM_REG_MSE[val_ids,:]).mean())\n",
    "print(abs(DIM_GT[val_ids,:]-DIM_REG_L1[val_ids,:]).mean())\n",
    "print(abs(DIM_GT[val_ids,:]-DIM_REG_AL1[val_ids,:]).mean())\n",
    "print('Train')\n",
    "print(abs(DIM_GT[train_ids,:]-DIM_REG_MSE[train_ids,:]).mean())\n",
    "print(abs(DIM_GT[train_ids,:]-DIM_REG_L1[train_ids,:]).mean())\n",
    "print(abs(DIM_GT[train_ids,:]-DIM_REG_AL1[train_ids,:]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "ac4c92fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.711701961317885\n",
      "0.13814717896766388\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(DIM_GT[:,:]-DIM_REG_L1[:,:]))\n",
    "print(abs(DIM_GT[:,:]-DIM_REG_L1[:,:]).mean()) #-335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "25164666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.81996391167859\n",
      "0.134547621557227\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(DIM_GT[:,1:]-DIM_REG_AL1[:,1:]))\n",
    "print(abs(DIM_GT[:,:]-DIM_REG_AL1[:,:]).mean()) #-542 -207"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
